# -*- coding: utf-8 -*-
"""Revised_book_recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Raghadzamil22/Recommend-system-/blob/main/Revised_book_recommendation_system.ipynb

# **Book Recommendation System::**
"""

pip install scikit-surprise

"""# Import The Required Libraries"""

# Libraries for data preparation
import numpy as np
import pandas as pd

# Importing libraries for model building & evaluation
from surprise import Reader, Dataset
from surprise.model_selection import train_test_split, cross_validate, GridSearchCV
from surprise import KNNBasic, KNNWithMeans
from surprise import accuracy

"""# Download The Datasets"""

book=pd.read_csv('/content/Books[1].csv')
rating = pd.read_csv('/content/Ratings[1].csv')

"""# Visulization And Exploration Of The Data"""

rating.shape

rating.head()

# Let's visualize the 'Book-Rating' column
rating['Book-Rating'].hist()

"""Ratings are of two types, an implicit rating & explicit rating. An implicit rating is based on tracking user interaction with an item such as a user clicking on an item and in this case is recorded as rating '0'. An explicit rating is when a user explicitly rates an item."""

book.shape

book.head()

"""# Data Preprocessing

There is inherent bias in the dataset. There are few users who rate a lot & several users that provide very few ratings. One user has provided 13K+ ratings, only ~700 users (out of 100K+ users) have provided over 250 ratings

A similar bias is observed. There are a few books that have received many ratings and several books that have received very few ratings. One book has received over 2500 ratings, only ~2100 books (out of 300K+ books) have received more than 50 ratings
"""

# In order to avoid rating bias & for making good recommendations, limit the dataset to only those
# users that have made at least 250 ratings & books that have received at least 50 ratings

user_ratings_count = rating['User-ID'].value_counts()
rating = rating[rating['User-ID'].isin(user_ratings_count[user_ratings_count >= 250].index)]
book_ratings_count = rating['ISBN'].value_counts()
rating =rating[rating['ISBN'].isin(book_ratings_count[book_ratings_count >= 50].index)]

# For the recommendation system, it is prefered to have the book titles rather than ISBN for easier interpretation

rating = rating.merge(book, on="ISBN")[['User-ID','Book-Title','Book-Rating']] # merging with the book dataframe

rating

# to avoid division by zero
rating = rating.drop(rating[rating['Book-Rating']<=0].index)
rating

# Check for duplicate values
print(f'Duplicate entries: {rating.duplicated().sum()}')

# drop duplicate values
rating.drop_duplicates(inplace=True)
rating

"""# Splitting The Data"""

# creating a surprise object

reader = Reader(rating_scale=(1, 10))
data   = Dataset.load_from_df(rating[['User-ID','Book-Title','Book-Rating']], reader)


# Split the data into training & testing sets.
raw_ratings = data.raw_ratings

import random
random.shuffle(raw_ratings)                 # shuffle dataset

threshold   = int(len(raw_ratings)*0.8)

train_raw_ratings = raw_ratings[:threshold] # 80% of data is trainset
test_raw_ratings  = raw_ratings[threshold:] # 20% of data is testset

data.raw_ratings = train_raw_ratings        # data is now the trainset
trainset         = data.build_full_trainset()
testset          = data.construct_testset(test_raw_ratings)

"""# Models Comparssion And Selection"""

# Trying KNN (K-Nearest Neighbors) using default model parameters

models=[KNNBasic(),KNNWithMeans()]
results = {}

for model in models:
    # perform 5 fold cross validation
    # evaluation metrics: mean absolute error & root mean square error
    CV_scores = cross_validate(model, data, measures=["MAE","RMSE"], cv=5, n_jobs=-1)

    # storing the average score across the 5 fold cross validation for each model
    result = pd.DataFrame.from_dict(CV_scores).mean(axis=0).\
             rename({'test_mae':'MAE', 'test_rmse': 'RMSE'})
    results[str(model).split("algorithms.")[1].split("object ")[0]] = result

performance_df = pd.DataFrame.from_dict(results)
print("Model Performance: \n")
performance_df.T.sort_values(by='RMSE')

"""# Hyperparameter tuning"""

# Hyperparameter tuning - KNNWithMeans

param_grid = { 'sim_options' : {'name': ['msd','cosine'],'min_support': [3,5], 'user_based': [False, True]}}

gridsearchKNNWithMeans = GridSearchCV(KNNWithMeans, param_grid, measures=['mae', 'rmse'],cv=10, n_jobs=-1)

gridsearchKNNWithMeans.fit(data)

print(f'MAE Best Parameters:  {gridsearchKNNWithMeans.best_params["mae"]}')
print(f'MAE Best Score:       {gridsearchKNNWithMeans.best_score["mae"]}\n')

print(f'RMSE Best Parameters: {gridsearchKNNWithMeans.best_params["rmse"]}')
print(f'RMSE Best Score:      {gridsearchKNNWithMeans.best_score["rmse"]}\n')

"""# Model Fitting & Prediction"""

# Model fit - KNNWithMeans

sim_options = {'name':'cosine','min_support':3,'user_based':False}
final_model = KNNWithMeans(sim_options=sim_options)

# Fitting the model on trainset
fitted_model = final_model.fit(trainset)

#Model prediction- KNNWithMeans
pred =fitted_model.test(testset)

"""# Model Performance Evaluation"""

print(f'\n Testing Performance:')
print(f'MAE: {accuracy.mae(pred)}, RMSE: {accuracy.rmse(pred)}')

"""**performane analysis of the model:**
Based on the comparison between our KNNWithMeans model and the k-NNBasic model, we can observe the following performance analysis of our model(KNNWithMeans):
1. Mean Absolute Error (MAE): The MAE value of our KNNWithMeans model is lower than the MAE value of the k-NNbasic model. This indicates that, on average, the predicted ratings of the KNNWithMeans model are closer to the actual ratings compared to the k-NNBasic model. A lower MAE value suggests better accuracy in predicting the ratings.
2. Root Mean Square Error (RMSE): The RMSE value of our KNNWithMeans model is lower than the RMSE value of the k-NNBasic model. This means that, on average, the predicted ratings of the KNNWithMeans model have a smaller squared difference from the actual ratings compared to the k-NNBasic model. A lower RMSE value indicates better accuracy in the predicted ratings.
Based on these metrics, it appears that our KNNWithMeans model outperforms the k-NNBasic model in terms of accuracy. It achieves lower MAE and RMSE values, indicating that it provides more accurate predictions and has a better fit to the data.

# Recommender Building
"""

# KNNWithMeans
# Entire dataset will be used for building recommendations

#list for holding the user feedback
list_of_feedback = []
def generate_recommendationsKNN(userID, like_recommend=5, get_recommend =10):


    ''' This function generates "get_recommend" number of book recommendations using
        KNNWithMeans & item based filtering. The function needs as input three
        different parameters:
        (1) userID i.e., userID for which recommendations need to be generated
        (2) like_recommend i.e., number of top recommendations for the userID to be
        considered for making recommendations
        (3) get_recommend i.e., number of recommendations to generate for the userID
        Default values are: like_recommend=5, get_recommend=10
        and userID will be inputed from the user
    '''
    #for checking user_input
    isfound=False
    for i  in rating['User-ID']:

      if i == userID:
       isfound=True
       break

    if isfound == False :
     print("Sorry your Id does not exist in our dataset could you please rate some books so we can help :)")
     return()

    # Compute item based similarity matrix
    sim_options       = {'name':'cosine','min_support':3,'user_based':False}
    similarity_matrix = fitted_model.compute_similarities()
    userID      = trainset.to_inner_uid(userID)    # converts the raw userID to innerID
    userRatings = trainset.ur[userID]              # method .ur takes user innerID &
                                                   # returns back a list of items and user ratings of this user


    # userRatings is a list of tuples [(,),(,),(,)..]. Each tuple contains item & rating
    # given by the user for that item. Next, the tuples will be sorted within the list
    # in decreasing order of rating. Then top 'like_recommend' items & ratings are extracted

    temp_df = pd.DataFrame(userRatings)
    temp_df = temp_df.sort_values(by=1, ascending=False)
    temp_df = temp_df.head(like_recommend)

    userRatings = temp_df.to_records(index=False) #converting the data frame into a structured array for farther processing

    # for each (item,rating) in top like_recommend user items, multiply the user rating for
    # the item with the similarity score (later is obtained from item similarity_matrix) for
    # all items. This helps calculate the weighted rating for all items. The weighted ratings
    # are added & divided by sum of weights to estimate rating the user would give an item
    recommendations   = {}

    for user_top_item, user_top_item_rating  in userRatings:

        all_item_indices          =   list(pd.DataFrame(similarity_matrix)[user_top_item].index)

        all_item_weighted_rating  =   list(pd.DataFrame(similarity_matrix)[user_top_item].values*user_top_item_rating)

        all_item_weights          =   list(pd.DataFrame(similarity_matrix)[user_top_item].values)

        # All items & final estimated ratings are added to a dictionary called recommendations

        for index in range(len(all_item_indices)):
            if index in recommendations:
                # sum of weighted ratings
                recommendations[index] += all_item_weighted_rating[index]
            else:
                recommendations[index]  = all_item_weighted_rating[index]


    for index in range(len(all_item_indices)):
            if all_item_weights[index]  !=0:
                # final ratings (sum of weighted ratings/sum of weights)
                recommendations[index]   =recommendations[index]/(all_item_weights[index]*like_recommend)


    # convert dictionary recommendations to a be a list of tuples [(,),(,),(,)]
    # with each tuple being an item & estimated rating user would give that item
    # sort the tuples within the list to be in decreasing order of estimated ratings

    temp_df = pd.Series(recommendations).reset_index().sort_values(by=0, ascending=False)
    recommendations = list(temp_df.to_records(index=False))

    # return get_recommend number of recommedations (only return items the user
    # has not previously rated)

    final_recommendations = []
    count = 0
    i=1
    for item, score in recommendations:
        flag = True
        for userItem, userRating in trainset.ur[userID]:
            if item == userItem:
                flag = False       # If item in recommendations has not been rated by user,
                break              # add to final_recommendations

        if flag == True:
          count +=1
          print(trainset.to_raw_iid(item))
          #take the user feedback for each book suggestion
          input_value = input('Do you like book number ' + str(i) + ':  (enter ''D'' for Dislike or ''L'' for Like) ').upper()
          i=i+1
          list_of_feedback.append(input_value)
                          # trainset has the items stored as inner id,
                                   # convert to raw id & append

        if count > get_recommend-1:  # Only get 'get_recommend' number of recommendations
            break

user_input=input("Please Enter your ID: ")# for test, use id no. 13552
generate_recommendationsKNN(int(user_input))

"""#Connecting the recommendation system with GPT 3.5"""

!pip install openai

from openai import OpenAI

client = OpenAI(
    api_key="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",
)

def display_chat_history(messages):
    for message in messages:
        print(f"{message['role'].capitalize()}: {message['content']}")

def get_assistant_response(messages):
    r = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": m["role"], "content": m["content"]} for m in messages],
    )
    response = r.choices[0].message.content
    return response

messages = [{"role":"assistant","content":" "}]

def GPT(user_input):
 while(user_input=='1'):
  print('what is your question: ')
  user = input("User: ")
  messages.append({"role": "user", "content":user})

  assistant_response = get_assistant_response(messages)
  print(assistant_response)

  user_input=input('do you want further information? enter 1 for yes, or 0 for no:')

user_input=input('do you want further information? enter 1 for yes, or 0 for no:')
if(user_input=='1'):
 GPT(user_input)
print("thank you! goodbye")

"""# Measuring the business performance"""

list_of_feedback

num_d = list_of_feedback.count('D')
if(num_d>=4):
 print("bad performance :(")
else:
 print("good performance :)")